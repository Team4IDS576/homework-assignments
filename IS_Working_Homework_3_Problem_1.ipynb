{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RNN for Language Modeling (4pt)\n",
    "\n",
    " -  Import the torchtext IMDB dataset and do the following:\n",
    "   -  Build a  Markov (n-gram) language model.\n",
    "   -  Change the output and the model appropriately in _[Simple Sentiment Analysis.ipynb](https://github.com/bentrevett/pytorch-sentiment-analysis)_ (also available [here](https://github.com/thejat/dl-notebooks/blob/master/examples/rnn/Seq2Seq_RNN_Simple_Sentiment_Analysis.ipynb) where the imports have been slightly modified) to build an LSTM based language model. Plot the training performance as a function of epochs/iterations.\n",
    " -  For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality.\n",
    " -  For each model, starting with the phrase \"My favorite movie \", sample the next few words and create an approx. 20 word generated review. Repeat this 5 times (you should ideally get different outputs each time) and report the outputs.\n",
    " - Note: make any assumptions as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `torchtext` IMDB dataset\n",
    "We begin by setting up the `torch` environment and downloading the `IMDB` dataset from the `torchtext.data.Dataset` class. The `torchtext` dataset is preprocessed into a into a collection of lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import mps\n",
    "\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# make torch deterministic for reproducibility\n",
    "seed = 576\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# set device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train model on training split\n",
    "train_iter = datasets.IMDB(split=\"train\")\n",
    "\n",
    "# initialize a list to store reviews\n",
    "reviews = []\n",
    "\n",
    "# append each review \n",
    "for _, review in train_iter:\n",
    "    reviews.append(review)\n",
    "\n",
    "# initialize list to store tokenized reviews\n",
    "tokenized_reviews = []\n",
    "\n",
    "# preprocess and tokenize reviews\n",
    "for review in reviews:\n",
    "    lowercase_review = review.lower()\n",
    "    cleaned_review = re.sub(r'[^A-Za-z0-9\\s]+', '', lowercase_review.replace('<br />', ' '))\n",
    "    tokenized_reviews.append(word_tokenize(cleaned_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov (n-gram) Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _Markov (n-gram)_ language model is a purely statistical character-level model. The model is based on the assumption that the probability of the next word in a sequence is based on the history _h_ of the preceding words in the sequence. For a set number _n_ of immediately previous words (or tokens) in the sequence, this assumption can be expressed as:\n",
    "\n",
    "$$P(w|h)\\approx P(w_n|w_{1:n-1})$$\n",
    "\n",
    "A _bigram_ is specific _n-gram_ case (_n_ = 2) where the conditional probability of the next word in a sequence is dependent solely on the preceding word in the sequence\n",
    "\n",
    "$$P(w|h)\\approx P(w_n|w_{n-1})$$\n",
    "\n",
    "We can further generalize this propoerty to _trigrams_ (_n_ = 3) such that the conditional probability of the next word in a sequence is \n",
    "\n",
    "$$P(w|h)\\approx P(w_n|w_{n-2, n-1})$$\n",
    "\n",
    "Let us create a general python class `build_random_ngram_model` that replicates the above funtionality with the IMBD dataset. In order to reduce recursive, repeating predicitons, the `predic()` method introduces randomness to the model, using a weighted choice between all tokens $w_n$ that correspond to a certain context $w_{1:n-1}$. The method parameter `rand` allows us to select the top `rand` weighted words from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "class build_random_ngram_model():\n",
    "    \n",
    "    '''\n",
    "    This class builds a Markov (n-gram) language model.\n",
    "    '''\n",
    "    def __init__(self, tokenized_reviews: list, n: int = 2):\n",
    "        self.n = n\n",
    "        self.model = self._build_model(tokenized_reviews, n=n)\n",
    "    \n",
    "    def _build_model(self, tokenized_reviews, n):\n",
    "        \n",
    "        # empty return model\n",
    "        model = {}\n",
    "        \n",
    "        # build model\n",
    "        print(\"builing model...\")\n",
    "        for review in tqdm(tokenized_reviews):\n",
    "            \n",
    "            # build n-grams from reviews\n",
    "            for ngram in ngrams(review, n, pad_right=True, pad_left=True):\n",
    "                context = tuple(ngram[:-1])\n",
    "                target = ngram[-1]\n",
    "                \n",
    "                # check if context is already in the model\n",
    "                if context not in model:\n",
    "                    model[context] = {}\n",
    "\n",
    "                # get frequency counts of co-occurrence\n",
    "                if target not in model[context]:\n",
    "                    model[context][target] = 1\n",
    "                else:\n",
    "                    model[context][target] += 1\n",
    "\n",
    "        # convert frequency counts to probabilities\n",
    "        for context in model:\n",
    "            count = float(sum(model[context].values()))\n",
    "            for target in model[context]:\n",
    "                model[context][target] /= count\n",
    "                \n",
    "        # return trained model\n",
    "        print(\"done!\")\n",
    "        return model\n",
    "    \n",
    "    def get_model(self) -> dict:\n",
    "        \n",
    "        '''\n",
    "        Returns model stored as dict.\n",
    "        '''\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def predict(self, words: str, rand: int = None) -> str:\n",
    "        \n",
    "        '''\n",
    "        This function returns the next word given the input string.\n",
    "        Randomly select out of the top rand probabilities given a certain context.\n",
    "        '''\n",
    "        \n",
    "        # get last n-1 words in string\n",
    "        key = tuple(words.split())[1-self.n:]\n",
    "        \n",
    "        # get top keys in dictionary\n",
    "        top_keys = sorted(self.model[key], key = self.model[key].get, reverse=True)[:rand]\n",
    "        \n",
    "        # get probs of top keys\n",
    "        top_values = [self.model[key][top_key] for top_key in top_keys]\n",
    "        \n",
    "        # select random key\n",
    "        selected_key = random.choices(top_keys, weights = top_values, k=1)[0]\n",
    "        \n",
    "        return selected_key\n",
    "    \n",
    "    def complete(self, words: str, rand: int = None) -> str:\n",
    "        \n",
    "        '''\n",
    "        This function returns the entire string with the predicted next word\n",
    "        '''\n",
    "        \n",
    "        next_word = self.predict(words, rand = rand)\n",
    "        \n",
    "        # if context does not exist in model dict return original words\n",
    "        if next_word is not None:\n",
    "            completed_string = words+\" \"+next_word\n",
    "        else:\n",
    "            completed_string = words\n",
    "        \n",
    "        return completed_string\n",
    "\n",
    "    def review(self, words: str, l = 10, rand: int = None) -> str:\n",
    "        \n",
    "        '''\n",
    "        This function completes a review for an additional l words.\n",
    "        '''\n",
    "        \n",
    "        working_words = words\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        while i < l:\n",
    "            working_words = self.complete(working_words, rand = rand)\n",
    "            i += 1\n",
    "            \n",
    "        return working_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build a trigram language model and generate random reviews from the input string `My favorite movie`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_model = build_random_ngram_model(tokenized_reviews, 3)\n",
    "\n",
    "i = 0\n",
    "while i < 5:\n",
    "    random_review = trigram_model.review(\"My favorite movie\", 17)\n",
    "    print(random_review)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Based Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the LSTM based language model we shall be following along with [Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb](https://github.com/thejat/dl-notebooks/blob/master/examples/rnn/Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb) and making modifications as neccessary.\n",
    "\n",
    "First we download the `\"train\"` and `\"test\"` splits from the `IMDB` dataset and check the size of each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  25000\n",
      "Test dataset size:  25000\n"
     ]
    }
   ],
   "source": [
    "train_dataset_raw = datasets.IMDB(split=\"train\")\n",
    "test_dataset_raw = datasets.IMDB(split=\"test\")\n",
    "\n",
    "print(\"Train dataset size: \",len(list(train_dataset_raw)))\n",
    "print(\"Test dataset size: \",len(list(test_dataset_raw)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "We shall use the `random_split()` utility to create train and validation sets from `test_dataset_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "\n",
    "test_dataset, val_dataset = random_split(\n",
    "    list(test_dataset_raw), [20000,5000], generator=generator\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline\n",
    "We next construct utilities to aid in the preprocessing of the `IMDB` dataset. Since we are creating a language model, our preprocessing pipeline must result in the generation of an _input sequence_ and _target sequence_. The _input sequence_ and _target sequence_ differ only by one \"time-step\".\n",
    "\n",
    "$$X_{Raw} =[x_1, x_2, \\ldots, x_T]$$\n",
    "$$X_{Input} = [x_1, x_2, \\ldots, x_{T-1}]$$\n",
    "$$X_{Target} = [x_2, x_3, \\ldots, x_{T}]$$\n",
    "\n",
    "The first utility is the `tokenizer()`, that parses through string instances in the datasets and converts to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n",
    "    # step 2: use lowercase for all text to keep symmetry\n",
    "    # step 3: extract emoticons. keep them as they are important sentiment signals\n",
    "    # step 4: remove punctuation marks\n",
    "    # step 5: put back emoticons\n",
    "    # step 6: generate word tokens\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    text = text.lower()\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+\", \" \", text)\n",
    "    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    tokenized = text.split()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next create a utility to obtain `token_counts` from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB train dataset vocab size: 75977\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "token_counts = Counter()\n",
    "\n",
    "for _, line in iter(train_dataset_raw):\n",
    "    tokens = tokenizer(line)\n",
    "    token_counts.update(tokens)\n",
    "\n",
    "print('IMDB train dataset vocab size:', len(token_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokens are then sorted by frequency and converted to to integers using the `vocab` object. For the sake of computational complexity the first 1,000 words of the ordered dictionary by freqeuency are used for the model vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this  -->  11\n",
      "is  -->  7\n",
      "an  -->  35\n",
      "example  -->  462\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "\n",
    "reduced_ordered_dict = OrderedDict()\n",
    "\n",
    "count = 0\n",
    "for key, value in ordered_dict.items():\n",
    "    reduced_ordered_dict[key] = value\n",
    "    count += 1\n",
    "    \n",
    "    if count == 500:\n",
    "        break\n",
    "\n",
    "vb = vocab(reduced_ordered_dict)\n",
    "\n",
    "vb.insert_token(\"<pad>\", 0)  # special token for padding\n",
    "vb.insert_token(\"<unk>\", 1)  # special token for unknown words\n",
    "vb.set_default_index(1)\n",
    "\n",
    "# print some token indexes from vocab\n",
    "for token in [\"this\", \"is\", \"an\", \"example\"]:\n",
    "    print(token, \" --> \", vb[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inline lambda functions are then used for text processing. This is the first major deviation from the source code; to create _input sequences_ and _target sequences_ from the raw dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Sequences\n",
      "Sample Tokens: ['i', 'rented', 'i', 'am', 'curious', 'yellow']\n",
      "Input Sequence: [10, 1, 10, 244, 1]\n",
      "Target Sequence: [1, 10, 244, 1, 1] \n",
      "\n",
      "Sample Tokens: ['i', 'am', 'curious', 'yellow', 'is', 'a']\n",
      "Input Sequence: [10, 244, 1, 1, 7]\n",
      "Target Sequence: [244, 1, 1, 7, 4] \n",
      "\n",
      "Sample Tokens: ['if', 'only', 'to', 'avoid', 'making', 'this']\n",
      "Input Sequence: [46, 64, 6, 1, 229]\n",
      "Target Sequence: [64, 6, 1, 229, 11] \n",
      "\n",
      "Sample Tokens: ['this', 'film', 'was', 'probably', 'inspired', 'by']\n",
      "Input Sequence: [11, 20, 14, 240, 1]\n",
      "Target Sequence: [20, 14, 240, 1, 34] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_pipeline = lambda x: [vb[token] for token in tokenizer(x)][:-1]\n",
    "target_pipline = lambda x: [vb[token] for token in tokenizer(x)][1:]\n",
    "\n",
    "length = 5\n",
    "print(\"Example Sequences\")\n",
    "for i in list(train_dataset_raw)[:4]:\n",
    "    example_tokens = tokenizer(i[1])[:length+1]\n",
    "    example_input = input_pipeline(i[1])[:length]\n",
    "    example_target = target_pipline(i[1])[:length]\n",
    "    \n",
    "    print(f'Sample Tokens: {example_tokens}')\n",
    "    print(f'Input Sequence: {example_input}')\n",
    "    print(f'Target Sequence: {example_target}', \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing utilities will be applied at the batch level, as a `collate_fn` argument. The source code is modified to return the input and target sequences. Since these sequences are of the same length, we do not need to return length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def collate_batch(batch):\n",
    "    input_list, target_list, = [], []\n",
    "\n",
    "    # iterate over all reviews in a batch\n",
    "    for _, _text in batch:\n",
    "\n",
    "        # input preprocessing\n",
    "        processed_input = torch.tensor(input_pipeline(_text), dtype=torch.int64)\n",
    "\n",
    "        # target preprocessing\n",
    "        processed_target = torch.tensor(input_pipeline(_text), dtype=torch.int64)\n",
    "\n",
    "\n",
    "        # store the processed text in input and target lists\n",
    "        input_list.append(processed_input)\n",
    "        target_list.append(processed_target)\n",
    "    \n",
    "    # pad the processed reviews to make their lengths consistant\n",
    "    padded_input_list = nn.utils.rnn.pad_sequence(\n",
    "        input_list, batch_first=True)\n",
    "    \n",
    "    padded_target_list = nn.utils.rnn.pad_sequence(\n",
    "        target_list, batch_first=True\n",
    "    )\n",
    "    \n",
    "    # return\n",
    "    # 1. a list of processed and padded input texts\n",
    "    # 2. a list of processed and padded target texts\n",
    "    # 3. a list of review text original lengths (before padding)\n",
    "    return padded_input_list.to(device), padded_target_list.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching the Training, Validation, and Test Datasets\n",
    "The `IMDB` Datasets are loaded into torch `DataLoader()` objects with the above `collate_batch()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_test = Subset(train_dataset_raw, list(range(5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_dataset_raw, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dl = DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset_raw, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now modify the source code's sentiment analysis `RNN` class for the purposes of language modeling. An overview of the architecture and design choices made to implement this are as follows:\n",
    "- An **Embedding layer** as as explained by the source code.\n",
    "- An **LSTM layer**  to capture long range dependencies and relationships in the text.\n",
    "- A **Fully Connected layer** to obtain `logits`, the raw unnormalized predictions for each token at each \"time step\" in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 33518\n"
     ]
    }
   ],
   "source": [
    "# create langauge model class\n",
    "class RNN_Language(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_sequence):\n",
    "        \n",
    "        out = self.embedding(input_sequence)\n",
    "        out, _ = self.rnn(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "    \n",
    "# instantiate a model\n",
    "vocab_size = len(vb)\n",
    "embed_dim = 20\n",
    "rnn_hidden_size = 32\n",
    "\n",
    "torch.manual_seed(576)\n",
    "model = RNN_Language(vocab_size, embed_dim, rnn_hidden_size)\n",
    "model = model.to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function, Optimizer and Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will utilize `CrossEntropyLoss()` as our loss function and the `Adam()` algorithm for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# training loop\n",
    "\n",
    "def train(model, dataloader):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for input_batch, target_batch in dataloader:\n",
    "            # forward pass\n",
    "            optimizer.zero_grad()\n",
    "            logits = model.forward(input_batch)\n",
    "            \n",
    "            loss = criterion(logits.view(-1, vocab_size), target_batch.view(-1))\n",
    "            \n",
    "            # backwards pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [03:14<12:57, 194.41s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/isaacsalvador/Documents/UIC/PhD/Fall 2023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m train(model, train_dl)\n",
      "\u001b[1;32m/Users/isaacsalvador/Documents/UIC/PhD/Fall 2023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m input_batch, target_batch \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# forward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(input_batch)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(logits\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, vocab_size), target_batch\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m# backwards pass\u001b[39;00m\n",
      "\u001b[1;32m/Users/isaacsalvador/Documents/UIC/PhD/Fall 2023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_sequence):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(input_sequence)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isaacsalvador/Documents/UIC/PhD/Fall%202023/git/homework-assignments/Working/IS_Working_Homework_3_Problem_1.ipynb#X54sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation Pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
