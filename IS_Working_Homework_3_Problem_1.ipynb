{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n13hAAC2wmgC"
      },
      "source": [
        "# 1. RNN for Language Modeling (4pt)\n",
        "\n",
        " -  Import the torchtext IMDB dataset and do the following:\n",
        "   -  Build a  Markov (n-gram) language model.\n",
        "   -  Change the output and the model appropriately in _[Simple Sentiment Analysis.ipynb](https://github.com/bentrevett/pytorch-sentiment-analysis)_ (also available [here](https://github.com/thejat/dl-notebooks/blob/master/examples/rnn/Seq2Seq_RNN_Simple_Sentiment_Analysis.ipynb) where the imports have been slightly modified) to build an LSTM based language model. Plot the training performance as a function of epochs/iterations.\n",
        " -  For each model, describe the key design choices made. Briefly mention how each choice influences training time and generative quality.\n",
        " -  For each model, starting with the phrase \"My favorite movie \", sample the next few words and create an approx. 20 word generated review. Repeat this 5 times (you should ideally get different outputs each time) and report the outputs.\n",
        " - Note: make any assumptions as necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjlnd8E4wmgF"
      },
      "source": [
        "## `torchtext` IMDB dataset\n",
        "We begin by setting up the `torch` environment and downloading the `IMDB` dataset from the `torchtext.data.Dataset` class. The `torchtext` dataset is preprocessed into a into a collection of lists of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6L_KGaQzwmgG",
        "outputId": "117a4a31-60b9-4240-c01d-c0d22767379d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import mps\n",
        "\n",
        "import torchtext\n",
        "from torchtext import datasets\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "# make torch deterministic for reproducibility\n",
        "seed = 576\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# set device\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Torch Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7R1JasZwmgG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# train model on training split\n",
        "train_iter = datasets.IMDB(split=\"train\")\n",
        "\n",
        "# initialize a list to store reviews\n",
        "reviews = []\n",
        "\n",
        "# append each review\n",
        "for _, review in train_iter:\n",
        "    reviews.append(review)\n",
        "\n",
        "# initialize list to store tokenized reviews\n",
        "tokenized_reviews = []\n",
        "\n",
        "# preprocess and tokenize reviews\n",
        "for review in reviews:\n",
        "    lowercase_review = review.lower()\n",
        "    cleaned_review = re.sub(r'[^A-Za-z0-9\\s]+', '', lowercase_review.replace('<br />', ' '))\n",
        "    tokenized_reviews.append(word_tokenize(cleaned_review))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHwcY2uywmgG"
      },
      "source": [
        "## Markov (n-gram) Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBr3ybo6wmgH"
      },
      "source": [
        "A _Markov (n-gram)_ language model is a purely statistical character-level model. The model is based on the assumption that the probability of the next word in a sequence is based on the history _h_ of the preceding words in the sequence. For a set number _n_ of immediately previous words (or tokens) in the sequence, this assumption can be expressed as:\n",
        "\n",
        "$$P(w|h)\\approx P(w_n|w_{1:n-1})$$\n",
        "\n",
        "A _bigram_ is specific _n-gram_ case (_n_ = 2) where the conditional probability of the next word in a sequence is dependent solely on the preceding word in the sequence\n",
        "\n",
        "$$P(w|h)\\approx P(w_n|w_{n-1})$$\n",
        "\n",
        "We can further generalize this propoerty to _trigrams_ (_n_ = 3) such that the conditional probability of the next word in a sequence is\n",
        "\n",
        "$$P(w|h)\\approx P(w_n|w_{n-2, n-1})$$\n",
        "\n",
        "Let us create a general python class `build_random_ngram_model` that replicates the above funtionality with the IMBD dataset. In order to reduce recursive, repeating predicitons, the `predic()` method introduces randomness to the model, using a weighted choice between all tokens $w_n$ that correspond to a certain context $w_{1:n-1}$. The method parameter `rand` allows us to select the top `rand` weighted words from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahhLQCfKwmgH"
      },
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "class build_random_ngram_model():\n",
        "\n",
        "    '''\n",
        "    This class builds a Markov (n-gram) language model.\n",
        "    '''\n",
        "    def __init__(self, tokenized_reviews: list, n: int = 2):\n",
        "        self.n = n\n",
        "        self.model = self._build_model(tokenized_reviews, n=n)\n",
        "\n",
        "    def _build_model(self, tokenized_reviews, n):\n",
        "\n",
        "        # empty return model\n",
        "        model = {}\n",
        "\n",
        "        # build model\n",
        "        print(\"builing model...\")\n",
        "        for review in tqdm(tokenized_reviews):\n",
        "\n",
        "            # build n-grams from reviews\n",
        "            for ngram in ngrams(review, n, pad_right=True, pad_left=True):\n",
        "                context = tuple(ngram[:-1])\n",
        "                target = ngram[-1]\n",
        "\n",
        "                # check if context is already in the model\n",
        "                if context not in model:\n",
        "                    model[context] = {}\n",
        "\n",
        "                # get frequency counts of co-occurrence\n",
        "                if target not in model[context]:\n",
        "                    model[context][target] = 1\n",
        "                else:\n",
        "                    model[context][target] += 1\n",
        "\n",
        "        # convert frequency counts to probabilities\n",
        "        for context in model:\n",
        "            count = float(sum(model[context].values()))\n",
        "            for target in model[context]:\n",
        "                model[context][target] /= count\n",
        "\n",
        "        # return trained model\n",
        "        print(\"done!\")\n",
        "        return model\n",
        "\n",
        "    def get_model(self) -> dict:\n",
        "\n",
        "        '''\n",
        "        Returns model stored as dict.\n",
        "        '''\n",
        "\n",
        "        return self.model\n",
        "\n",
        "    def predict(self, words: str, rand: int = None) -> str:\n",
        "\n",
        "        '''\n",
        "        This function returns the next word given the input string.\n",
        "        Randomly select out of the top rand probabilities given a certain context.\n",
        "        '''\n",
        "\n",
        "        # get last n-1 words in string\n",
        "        key = tuple(words.split())[1-self.n:]\n",
        "\n",
        "        # get top keys in dictionary\n",
        "        top_keys = sorted(self.model[key], key = self.model[key].get, reverse=True)[:rand]\n",
        "\n",
        "        # get probs of top keys\n",
        "        top_values = [self.model[key][top_key] for top_key in top_keys]\n",
        "\n",
        "        # select random key\n",
        "        selected_key = random.choices(top_keys, weights = top_values, k=1)[0]\n",
        "\n",
        "        return selected_key\n",
        "\n",
        "    def complete(self, words: str, rand: int = None) -> str:\n",
        "\n",
        "        '''\n",
        "        This function returns the entire string with the predicted next word\n",
        "        '''\n",
        "\n",
        "        next_word = self.predict(words, rand = rand)\n",
        "\n",
        "        # if context does not exist in model dict return original words\n",
        "        if next_word is not None:\n",
        "            completed_string = words+\" \"+next_word\n",
        "        else:\n",
        "            completed_string = words\n",
        "\n",
        "        return completed_string\n",
        "\n",
        "    def review(self, words: str, l = 10, rand: int = None) -> str:\n",
        "\n",
        "        '''\n",
        "        This function completes a review for an additional l words.\n",
        "        '''\n",
        "\n",
        "        working_words = words\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        while i < l:\n",
        "            working_words = self.complete(working_words, rand = rand)\n",
        "            i += 1\n",
        "\n",
        "        return working_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WukicHnCwmgH"
      },
      "source": [
        "We can now build a trigram language model and generate random reviews from the input string `My favorite movie`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGKiwSHcwmgH"
      },
      "outputs": [],
      "source": [
        "trigram_model = build_random_ngram_model(tokenized_reviews, 3)\n",
        "\n",
        "i = 0\n",
        "while i < 5:\n",
        "    random_review = trigram_model.review(\"My favorite movie\", 17)\n",
        "    print(random_review)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEbZAa21wmgH"
      },
      "source": [
        "## LSTM Based Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9tsMapywmgI"
      },
      "source": [
        "For the LSTM based language model we shall be following along with [Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb](https://github.com/thejat/dl-notebooks/blob/master/examples/rnn/Seq2Seq_LSTM_Simple_Sentiment_Analysis.ipynb) and making modifications as neccessary.\n",
        "\n",
        "First we download the `\"train\"` and `\"test\"` splits from the `IMDB` dataset and check the size of each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cKXCW4ODwmgI",
        "outputId": "a31df1b3-009c-496d-9c42-f8ce565d6b1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size:  25000\n",
            "Test dataset size:  25000\n"
          ]
        }
      ],
      "source": [
        "train_dataset_raw = datasets.IMDB(split=\"train\")\n",
        "test_dataset_raw = datasets.IMDB(split=\"test\")\n",
        "\n",
        "print(\"Train dataset size: \",len(list(train_dataset_raw)))\n",
        "print(\"Test dataset size: \",len(list(test_dataset_raw)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9kGMzkpwmgI"
      },
      "source": [
        "### Data Preprocessing Pipeline\n",
        "We next construct utilities to aid in the preprocessing of the `IMDB` dataset. Since we are creating a language model, our preprocessing pipeline must result in the generation of an _input sequence_ and _target sequence_. The _input sequence_ and _target sequence_ differ only by one \"time-step\".\n",
        "\n",
        "$$X_{Raw} =[x_1, x_2, \\ldots, x_T]$$\n",
        "$$X_{Input} = [x_1, x_2, \\ldots, x_{T-1}]$$\n",
        "$$X_{Target} = [x_2, x_3, \\ldots, x_{T}]$$\n",
        "\n",
        "The first utility is the `tokenizer()`, that parses through string instances in the datasets and converts to tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ShicIa1VwmgJ"
      },
      "outputs": [],
      "source": [
        "def tokenizer(text):\n",
        "    # step 1. remove HTML tags. they are not helpful in understanding the sentiments of a review\n",
        "    # step 2: use lowercase for all text to keep symmetry\n",
        "    # step 3: extract emoticons. keep them as they are important sentiment signals\n",
        "    # step 4: remove punctuation marks\n",
        "    # step 5: put back emoticons\n",
        "    # step 6: generate word tokens\n",
        "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
        "    text = text.lower()\n",
        "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
        "    text = re.sub(\"[\\W]+\", \" \", text)\n",
        "    text = text + \" \".join(emoticons).replace(\"-\", \"\")\n",
        "    tokenized = text.split()\n",
        "    return tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHrhjm2fwmgJ"
      },
      "source": [
        "We next create a utility to obtain `token_counts` from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TKbTqwp5wmgJ",
        "outputId": "01231c93-6a14-4450-bbef-0b1a6ac47db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMDB train dataset vocab size: 75977\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "for _, line in iter(train_dataset_raw):\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        "\n",
        "print('IMDB train dataset vocab size:', len(token_counts))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLsWbXdBwmgJ"
      },
      "source": [
        "The tokens are then sorted by frequency and converted to to integers using the `vocab` object. For the sake of computational complexity the first 500 words of the ordered dictionary by freqeuency are used for the model vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-SFK-YFRwmgK",
        "outputId": "f38077a6-8c17-4399-c431-31a02db7ac55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "this  -->  11\n",
            "is  -->  7\n",
            "an  -->  35\n",
            "example  -->  462\n"
          ]
        }
      ],
      "source": [
        "from collections import OrderedDict\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "reduced_ordered_dict = OrderedDict()\n",
        "\n",
        "count = 0\n",
        "for key, value in ordered_dict.items():\n",
        "    reduced_ordered_dict[key] = value\n",
        "    count += 1\n",
        "\n",
        "    if count == 500:\n",
        "        break\n",
        "\n",
        "vb = vocab(reduced_ordered_dict)\n",
        "\n",
        "vb.insert_token(\"<pad>\", 0)  # special token for padding\n",
        "vb.insert_token(\"<unk>\", 1)  # special token for unknown words\n",
        "vb.set_default_index(1)\n",
        "\n",
        "# print some token indexes from vocab\n",
        "for token in [\"this\", \"is\", \"an\", \"example\"]:\n",
        "    print(token, \" --> \", vb[token])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7HL7e6twmgK"
      },
      "source": [
        "Inline lambda functions are then used for text processing. This is the first major deviation from the source code; to create _input sequences_ and _target sequences_ from the raw dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FMli58Z4wmgK",
        "outputId": "11e7f65b-81d1-4fdb-a99a-53c0b8dd9234",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example Sequences\n",
            "Sample Tokens: ['i', 'rented', 'i', 'am', 'curious', 'yellow']\n",
            "Input Sequence: [10, 1, 10, 244, 1]\n",
            "Target Sequence: [1, 10, 244, 1, 1] \n",
            "\n",
            "Sample Tokens: ['i', 'am', 'curious', 'yellow', 'is', 'a']\n",
            "Input Sequence: [10, 244, 1, 1, 7]\n",
            "Target Sequence: [244, 1, 1, 7, 4] \n",
            "\n",
            "Sample Tokens: ['if', 'only', 'to', 'avoid', 'making', 'this']\n",
            "Input Sequence: [46, 64, 6, 1, 229]\n",
            "Target Sequence: [64, 6, 1, 229, 11] \n",
            "\n",
            "Sample Tokens: ['this', 'film', 'was', 'probably', 'inspired', 'by']\n",
            "Input Sequence: [11, 20, 14, 240, 1]\n",
            "Target Sequence: [20, 14, 240, 1, 34] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "input_pipeline = lambda x: [vb[token] for token in tokenizer(x)][:-1]\n",
        "target_pipline = lambda x: [vb[token] for token in tokenizer(x)][1:]\n",
        "\n",
        "length = 5\n",
        "print(\"Example Sequences\")\n",
        "for i in list(train_dataset_raw)[:4]:\n",
        "    example_tokens = tokenizer(i[1])[:length+1]\n",
        "    example_input = input_pipeline(i[1])[:length]\n",
        "    example_target = target_pipline(i[1])[:length]\n",
        "\n",
        "    print(f'Sample Tokens: {example_tokens}')\n",
        "    print(f'Input Sequence: {example_input}')\n",
        "    print(f'Target Sequence: {example_target}', \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejDoESUDwmgK"
      },
      "source": [
        "The preprocessing utilities will be applied at the batch level, as a `collate_fn` argument. The source code is modified to return the input and target sequences. Since these sequences are of the same length, we do not need to return length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H0v862hHwmgK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def collate_batch(batch):\n",
        "    input_list, target_list, = [], []\n",
        "\n",
        "    # iterate over all reviews in a batch\n",
        "    for _, _text in batch:\n",
        "\n",
        "        # input preprocessing\n",
        "        processed_input = torch.tensor(input_pipeline(_text), dtype=torch.int64)\n",
        "\n",
        "        # target preprocessing\n",
        "        processed_target = torch.tensor(input_pipeline(_text), dtype=torch.int64)\n",
        "\n",
        "\n",
        "        # store the processed text in input and target lists\n",
        "        input_list.append(processed_input)\n",
        "        target_list.append(processed_target)\n",
        "\n",
        "    # pad the processed reviews to make their lengths consistant\n",
        "    padded_input_list = nn.utils.rnn.pad_sequence(\n",
        "        input_list, batch_first=True)\n",
        "\n",
        "    padded_target_list = nn.utils.rnn.pad_sequence(\n",
        "        target_list, batch_first=True\n",
        "    )\n",
        "\n",
        "    # return\n",
        "    # 1. a list of processed and padded input texts\n",
        "    # 2. a list of processed and padded target texts\n",
        "    # 3. a list of review text original lengths (before padding)\n",
        "    return padded_input_list.to(device), padded_target_list.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1K4h4pKwmgK"
      },
      "source": [
        "### Batching the Training, Validation, and Test Datasets\n",
        "The `IMDB` Datasets are loaded into torch `DataLoader()` objects with the above `collate_batch()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "J39b4RWcwmgL"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Subset\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(\n",
        "    train_dataset_raw, batch_size=batch_size, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    test_dataset_raw, batch_size=batch_size, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIGy9elVwmgL"
      },
      "source": [
        "### RNN Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOSjaoDwmgL"
      },
      "source": [
        "We will now modify the source code's sentiment analysis `RNN` class for the purposes of language modeling. An overview of the architecture and design choices made to implement this are as follows:\n",
        "- An **Embedding layer** as as explained by the source code.\n",
        "- An **LSTM layer**  to capture long range dependencies and relationships in the text.\n",
        "- A **Fully Connected layer** to obtain `logits`, the raw unnormalized predictions for each token at each \"time step\" in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7JC4_rFTwmgL",
        "outputId": "2926d1f1-bd08-4f47-8af2-3ace9c1f5522",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 64686\n"
          ]
        }
      ],
      "source": [
        "# create langauge model class\n",
        "class RNN_Language(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_sequence):\n",
        "\n",
        "        out = self.embedding(input_sequence)\n",
        "        out, _ = self.rnn(out)\n",
        "        logits = self.fc(out)\n",
        "        return logits\n",
        "\n",
        "# instantiate a model\n",
        "vocab_size = len(vb)\n",
        "embed_dim = 20\n",
        "rnn_hidden_size = 64\n",
        "\n",
        "torch.manual_seed(576)\n",
        "model = RNN_Language(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUUITVWuwmgL"
      },
      "source": [
        "### Loss Function, Optimizer and Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRBt6owewmgM"
      },
      "source": [
        "We will utilize `CrossEntropyLoss()` as our loss function and the `Adam()` algorithm for optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ZhU3s-mHwmgM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# training loop\n",
        "\n",
        "def train(model, train_dl, valid_dl):\n",
        "\n",
        "    train_history, valid_history = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(f\"Epopch: {epoch}\")\n",
        "\n",
        "        model.train()\n",
        "        total_loss, valid_loss = 0, 0\n",
        "\n",
        "        # training\n",
        "        for input_batch, target_batch in tqdm(train_dl):\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # forward pass\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(input_batch)\n",
        "                loss = criterion(logits.view(-1, vocab_size), target_batch.view(-1))\n",
        "\n",
        "                # backwards pass\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            total_loss += loss.item() * batch_size\n",
        "\n",
        "        avg_loss = total_loss / 25_000\n",
        "        train_history.append(avg_loss)\n",
        "        print(f\"Training loss:\\t{avg_loss}\")\n",
        "\n",
        "        # validation\n",
        "        for input_batch, target_batch in valid_dl:\n",
        "            with torch.no_grad():\n",
        "                logits = model(input_batch)\n",
        "                loss = criterion(logits.view(-1, vocab_size), target_batch.view(-1))\n",
        "\n",
        "            valid_loss += loss.item() * batch_size\n",
        "\n",
        "        avg_valid_loss = valid_loss / 25_000\n",
        "        valid_history.append(valid_loss)\n",
        "        print(f\"Validation Loss\\t{avg_valid_loss}\\n\")\n",
        "\n",
        "    return train_history, valid_history\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Mi3G7B6FwmgM",
        "outputId": "5b657066-e77f-412d-9d87-f3fbce4a6d1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epopch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "782it [00:29, 26.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:\t0.7904607945537567\n",
            "Validation Loss\t0.08984458872795105\n",
            "\n",
            "Epopch: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "782it [00:28, 27.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:\t0.029446056988239287\n",
            "Validation Loss\t0.009146965028345585\n",
            "\n",
            "Epopch: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "782it [00:28, 27.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:\t0.0052670444007217885\n",
            "Validation Loss\t0.0032305193048715593\n",
            "\n",
            "Epopch: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "782it [00:29, 26.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:\t0.0022146962712705134\n",
            "Validation Loss\t0.001600130711942911\n",
            "\n",
            "Epopch: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "782it [00:35, 22.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss:\t0.0011674381231144071\n",
            "Validation Loss\t0.0009092762515321374\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_history, valid_history = train(model, train_dl, valid_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6CuxuxPwmgM"
      },
      "source": [
        "### Training and Evaluation Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, text, max = 17):\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max):\n",
        "\n",
        "        text_tokens = [vb[token] for token in tokenizer(text)]\n",
        "        tensor_tokens = torch.tensor(text_tokens, dtype = torch.int64).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(tensor_tokens)[-1,:]\n",
        "\n",
        "        next_word_idx = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        next_word = vb.lookup_token(next_word_idx)\n",
        "\n",
        "        text = text+\" \"+next_word\n",
        "        print(text)\n",
        "\n",
        "    return text\n",
        "\n"
      ],
      "metadata": {
        "id": "I8HS4PZ86DKo"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, \"My favorite movie is\", 17)"
      ],
      "metadata": {
        "id": "88Frt8MDDdh8",
        "outputId": "7954c4c1-a467-419a-d9b1-3814b528b1cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My favorite movie is is\n",
            "My favorite movie is is is\n",
            "My favorite movie is is is is\n",
            "My favorite movie is is is is is\n",
            "My favorite movie is is is is is is\n",
            "My favorite movie is is is is is is is\n",
            "My favorite movie is is is is is is is is\n",
            "My favorite movie is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is is is is is is\n",
            "My favorite movie is is is is is is is is is is is is is is is is is is\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My favorite movie is is is is is is is is is is is is is is is is is is'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"My favorite movie\"\n",
        "\n",
        "text_tokens = [vb[token] for token in tokenizer(text)]\n",
        "\n",
        "tensor_tokens = torch.tensor(text_tokens, dtype = torch.int64).to(device)\n",
        "\n",
        "text_out = tensor_tokens.tolist()\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(tensor_tokens)\n",
        "\n",
        "    probs = logits # nn.Softmax(dim=1)(logits)\n",
        "\n",
        "next_word_idx = torch.argmax(probs[-1:], dim=-1)\n",
        "\n",
        "next_word = vb.lookup_token(next_word_idx)\n",
        "\n",
        "text\n",
        "\n"
      ],
      "metadata": {
        "id": "iWV_omuw62mR",
        "outputId": "230b37d3-266d-458f-f45e-e3b627569e02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'My favorite movie'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "id": "KOYsZIJ0FLXb",
        "outputId": "ee84bce7-14d2-4219-d627-0aa9de06c13e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7386,  0.6901,  2.0016,  ..., -2.7133, -4.2817, -0.6536],\n",
              "        [ 2.4042, 11.7791,  3.6418,  ..., -0.8284, -3.8727, -1.8953],\n",
              "        [ 2.4859, -1.0814,  2.9241,  ..., -0.3370, -6.7716, -0.4624]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits[-1,:]"
      ],
      "metadata": {
        "id": "uZ_tVYOXFbgb",
        "outputId": "35794459-9771-4420-e913-ab0ac3b3a008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.4859e+00, -1.0814e+00,  2.9241e+00,  4.1768e+00,  7.8067e-01,\n",
              "         2.1056e+00, -4.4439e+00,  5.5024e+00,  3.2304e+00,  4.7833e+00,\n",
              "        -2.7783e-01,  5.9671e+00, -3.1110e-01, -3.4999e+00,  4.1280e+00,\n",
              "        -4.4741e+00, -1.2282e+00, -2.2853e+00,  1.5474e+01, -4.1002e+00,\n",
              "        -1.2138e+00, -8.3343e+00,  1.5151e+00,  6.4328e+00,  5.0651e+00,\n",
              "        -1.2510e+00,  4.0658e-01, -2.9876e+00, -2.1848e+00, -2.5550e+00,\n",
              "         2.2466e+00,  8.7471e-01,  3.4491e+00, -1.4833e+00,  4.0507e-01,\n",
              "        -5.4572e+00, -4.9692e+00, -4.7724e+00, -5.7736e+00,  3.3365e+00,\n",
              "        -2.0869e+00, -3.0094e+00, -4.9603e+00, -3.2995e+00, -2.5502e+00,\n",
              "         6.0888e+00,  2.7601e+00, -2.9485e+00,  4.8214e+00, -6.7508e-01,\n",
              "         1.5197e+00,  3.5718e+00, -2.5319e+00, -8.9040e-01, -3.4544e+00,\n",
              "        -5.8472e+00, -2.0012e+00,  3.3882e+00, -1.5072e+00,  1.3887e+00,\n",
              "         6.7300e+00,  4.1304e+00, -5.5442e+00, -5.9565e+00,  4.6351e-01,\n",
              "        -5.2537e+00, -4.0805e+00,  7.4855e-01, -2.5820e+00, -1.4842e+00,\n",
              "         9.0662e-01, -3.2202e+00,  1.2431e+00, -1.1368e+00, -2.1693e+00,\n",
              "        -2.2106e+00, -2.5909e-01, -6.1067e+00,  4.4235e+00, -9.0331e-01,\n",
              "        -1.4076e+00,  1.1391e+00, -6.4210e+00, -2.5790e+00, -5.2239e+00,\n",
              "        -1.0062e+00, -7.0786e+00, -1.0147e+00, -8.0514e-01,  2.7944e+00,\n",
              "         3.1232e+00, -1.0109e+01, -6.0343e+00, -5.2509e+00, -6.9827e+00,\n",
              "        -3.4590e-01, -2.3927e+00, -4.1225e+00, -5.7907e+00,  2.8572e+00,\n",
              "         7.5074e-01, -1.4071e+00, -4.1309e+00,  6.5745e+00,  4.6934e+00,\n",
              "        -5.0315e+00,  1.6684e-01,  2.0115e+00, -1.7737e+00, -1.3066e+00,\n",
              "        -8.3054e+00, -4.9447e+00,  2.8137e+00,  4.2934e+00,  3.2900e+00,\n",
              "        -1.3717e+00, -3.1353e+00, -6.4498e+00, -5.9928e+00,  3.3357e+00,\n",
              "        -2.7434e-01, -2.4248e+00, -7.6383e-01, -1.3048e+00,  2.9306e+00,\n",
              "        -9.4275e-01,  1.7847e+00, -5.8120e+00, -6.5456e+00,  1.1616e+00,\n",
              "        -1.2156e+00, -4.4049e+00, -4.1485e+00, -4.5723e+00, -1.0930e+01,\n",
              "        -7.7191e+00,  1.5091e+00, -3.7056e+00, -3.0537e+00, -1.3697e+00,\n",
              "        -7.3966e-01, -5.4816e+00, -1.4494e+00, -1.0723e+00,  1.2077e+00,\n",
              "         2.0526e+00,  1.3343e+00, -1.0533e-02,  8.4334e-01, -7.5868e+00,\n",
              "        -4.9131e+00,  1.9010e+00, -6.5631e-01, -4.4576e+00,  3.9080e-01,\n",
              "        -3.3923e+00, -3.2209e+00,  5.8555e-01,  4.1975e+00,  3.0502e+00,\n",
              "        -7.3997e+00,  1.4389e+00,  1.5144e+00, -6.1478e+00, -2.9383e+00,\n",
              "        -2.2480e-01, -2.8367e+00, -5.5625e-01, -7.2331e+00, -9.7859e+00,\n",
              "        -4.9173e+00, -3.4671e+00, -6.3024e+00,  1.4868e+00, -2.4086e+00,\n",
              "        -1.9555e+00,  4.0645e+00, -8.6114e+00,  4.1545e-01, -4.6424e-01,\n",
              "        -3.1657e+00,  4.2487e-01, -4.8509e+00, -5.4570e+00, -3.8495e+00,\n",
              "        -1.7295e+00,  9.9398e-01, -1.4797e+00, -3.8332e+00,  1.2951e+00,\n",
              "        -4.3596e+00, -1.4907e+00, -2.1971e+00, -6.2873e-01,  3.8528e+00,\n",
              "        -3.8458e-01,  5.0463e-01, -2.8304e+00,  6.0928e-02, -3.2854e+00,\n",
              "         1.7404e+00, -1.4110e+00, -3.3359e+00, -3.1074e+00,  6.5670e-01,\n",
              "         1.4845e+00, -6.0507e+00, -7.1500e+00, -8.2399e+00, -8.6308e+00,\n",
              "         4.3548e+00,  2.8039e+00, -2.5718e+00,  6.6400e-02, -5.0850e+00,\n",
              "        -2.8629e+00, -3.7969e+00,  1.9716e+00, -1.5072e+00,  5.8293e-01,\n",
              "         7.5879e-01, -9.5547e+00,  2.9712e+00,  1.1619e+00, -4.6676e-01,\n",
              "         1.9213e+00, -2.1717e+00,  1.9699e+00, -7.0605e-01,  1.5991e+00,\n",
              "        -3.2744e+00,  2.1964e+00,  2.9540e+00, -2.2060e+00, -5.5773e+00,\n",
              "        -4.0208e+00, -1.4673e+00,  2.3210e+00,  1.1157e+00,  1.5587e+00,\n",
              "         2.5078e+00, -3.8987e+00, -2.4222e+00,  9.6923e-01, -9.8446e-01,\n",
              "        -8.2777e+00, -3.3158e+00, -2.8177e+00, -6.0085e+00, -2.1040e+00,\n",
              "         1.1761e+00,  2.9162e+00,  1.2534e+00, -1.3584e+00, -1.4870e+00,\n",
              "        -2.1676e+00, -1.3613e+00, -3.6766e+00, -6.0428e-02, -9.5976e-02,\n",
              "         4.0182e-01,  3.5978e+00, -4.9672e+00, -7.3318e+00, -1.0308e+01,\n",
              "         5.4364e-01, -2.1025e+00, -5.0378e+00, -1.3278e+00,  3.8764e-01,\n",
              "        -4.0616e+00, -4.3226e+00, -2.3237e+00, -4.4522e+00, -2.1687e+00,\n",
              "         1.3879e+00, -6.7430e+00, -4.8200e+00,  3.3681e+00, -5.8865e+00,\n",
              "        -2.7101e+00,  8.2269e-01, -7.3456e+00, -3.2915e+00, -2.1748e+00,\n",
              "        -1.9900e+00, -7.4856e+00,  1.5009e+00, -5.6332e-01,  4.2298e-02,\n",
              "         3.0627e+00, -1.7042e+00, -2.2872e+00, -3.5247e+00, -4.5348e+00,\n",
              "         9.5451e-01, -2.1814e-01, -4.2231e+00,  3.4809e+00, -1.3822e+00,\n",
              "         7.7828e-01,  1.5959e+00, -3.4483e+00, -6.2758e-01, -3.9376e+00,\n",
              "         3.1842e+00,  1.6545e+00, -1.7889e+00, -4.3379e+00, -7.0234e-01,\n",
              "        -1.5213e+00,  3.9788e+00, -1.1099e+01,  1.8764e-01, -5.8247e+00,\n",
              "         1.9528e+00,  1.1126e+00, -5.0506e+00, -5.1163e+00, -3.8552e-01,\n",
              "        -6.9832e+00, -1.3936e+00, -7.2594e+00, -1.9686e+00, -2.7283e+00,\n",
              "         4.0462e+00, -2.8315e+00, -3.2638e-01,  6.0300e-01, -3.6431e+00,\n",
              "         3.7105e-02,  2.9800e+00, -3.6498e+00,  1.1643e+00, -4.4038e+00,\n",
              "        -5.7772e-01, -4.2273e+00, -2.9917e+00, -1.0416e+01, -5.3391e+00,\n",
              "        -5.6867e+00,  1.7232e-01, -2.2255e+00, -3.4833e+00, -1.4100e+00,\n",
              "        -3.2394e+00,  4.9526e-01, -5.3840e+00, -2.5322e+00,  1.4840e+00,\n",
              "        -5.0950e+00,  3.3504e+00,  2.5804e+00,  4.6154e-01, -1.6013e+00,\n",
              "         4.1445e+00, -5.5175e+00, -7.2439e-01, -3.3432e+00, -8.0347e+00,\n",
              "         1.6999e+00,  1.3587e+00,  3.4345e+00, -8.7051e-01, -2.1630e+00,\n",
              "        -3.4304e+00, -5.7332e+00,  1.3800e+00, -1.5829e+00, -9.5667e+00,\n",
              "        -6.1928e-01, -2.2311e+00, -3.8990e+00,  1.1855e+00, -2.8389e+00,\n",
              "        -1.8378e+00,  3.9354e-01, -5.5309e+00, -9.1091e+00, -5.2691e-01,\n",
              "        -1.9637e+00, -6.0469e-01, -2.9398e+00, -1.4329e+00,  3.4114e+00,\n",
              "        -7.7219e+00, -4.7586e+00, -2.5691e+00, -4.4974e+00, -2.1157e+00,\n",
              "        -5.8937e+00,  2.0706e+00,  5.4448e+00, -1.5458e+00,  3.5281e+00,\n",
              "        -8.1939e-01, -5.9105e+00, -1.9095e+00, -3.0810e+00, -3.9827e+00,\n",
              "         1.8522e+00, -8.7444e-01, -6.0172e+00, -6.3312e+00,  5.6151e-01,\n",
              "        -5.9803e+00, -3.5088e+00, -2.3467e+00, -2.8031e-01, -3.8357e+00,\n",
              "        -3.9701e+00, -3.1940e+00,  7.2420e-01, -4.9314e-01, -4.9830e+00,\n",
              "         1.1860e+00, -2.1392e+00, -2.3901e+00, -1.4383e+00,  4.9933e+00,\n",
              "        -3.4652e+00, -2.4900e+00, -8.0536e+00,  3.3032e+00,  3.7673e-01,\n",
              "         2.5424e+00, -3.7418e+00,  2.0081e+00, -6.2402e+00, -1.7146e+00,\n",
              "        -1.7404e+00, -3.7879e-01, -1.2424e+00, -6.2315e+00,  2.9989e-01,\n",
              "         3.5379e+00, -6.2416e+00, -9.7116e+00, -1.3453e+00,  9.1157e-01,\n",
              "         2.5382e+00, -3.3503e+00, -6.8059e-01, -3.7465e+00, -5.0875e+00,\n",
              "        -2.2801e+00, -2.5361e+00, -3.6278e-01, -1.6977e+00, -2.9184e+00,\n",
              "         8.4872e-01,  1.4662e+00, -1.2708e+00,  4.6101e+00,  1.1346e+00,\n",
              "        -7.6182e+00,  2.0116e-01,  4.3412e+00,  5.4185e-01,  1.1072e+00,\n",
              "         4.9522e+00, -3.8455e+00, -2.3450e+00,  7.7694e-01, -6.3653e+00,\n",
              "        -2.8762e+00, -3.6882e+00,  2.5602e+00, -1.3528e+00, -1.6121e+00,\n",
              "        -1.1364e+00, -4.4620e+00, -1.4393e+00,  2.1553e+00, -4.9528e+00,\n",
              "        -2.2084e+00,  9.6416e-01,  8.1061e-01,  1.2907e+00,  2.4652e+00,\n",
              "        -6.0592e+00, -3.7056e+00, -7.1935e-01,  1.9604e+00, -5.0255e+00,\n",
              "        -4.3189e+00,  2.6364e+00, -1.4570e+00, -2.4818e+00,  2.5745e+00,\n",
              "        -6.8104e+00, -1.2468e+00, -8.7023e-01,  1.8607e+00, -3.9491e+00,\n",
              "        -1.0817e+00, -9.1561e-01,  3.2756e+00, -5.0908e+00, -3.3701e-01,\n",
              "        -6.7716e+00, -4.6238e-01], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}